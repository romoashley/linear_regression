---
title: "bootstrapping"
author: "ASHLEY ROMO"
date: "2023-11-26"
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Load key packages.

```{r}
library(tidyverse)
library(modelr)

set.seed(1)
```


## Generate a relevant sample

```{r}
n_samp = 250

#every residual is drawn from a normal distribution with mean 0 and sd 1
sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

# every residual depends on x (x values close to zero will have smaller errors and x values close to one will have larger errors)
sim_df_nonconst = 
  sim_df_const |> 
  mutate(
    error = error * 0.75 * x, 
    y = 2 + 3 * x + error
  )

sim_df_const |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point()

sim_df_nonconst |> 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

fit some linear models
```{r}
sim_df_const |> 
  lm(y ~ x, data = _) |> 
  broom::tidy()

sim_df_nonconst |> 
  lm(y ~ x, data = _) |> 
  broom::tidy() 

# lm assumes the assumptions of linear regression: aka that there is constant variance across the domain even when the data truly does not have constant variance 
# we need to do in the setting of nonconstant variance is deal with it in a way that allows us to get accurate confidence intervals despite the fact that the generating process is not what we assumed theoretically
```

## Draw and analyze a bootstrap sample

Start with a little function
```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE)
}
```

Let's see how this works
```{r}
sim_df_nonconst |> 
  boot_sample() |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = 0.5) +
  stat_smooth(method = "lm")
```

## Draw a lot of samples and analyze them 
```{r}
# 100 boostraps, fit a linear model to those, take a the estimated intercept and slopes for each of those, and look at the distribution of those 

# for each iteration of i in strap number, run boot_sample function on sim_df_nonconst

boot_straps = 
  tibble(strap_number = 1:1000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(sim_df_nonconst)))


#sample to sample variability (different bootstrap sample each time you do it)
boot_straps |> 
  pull(strap_sample) |> 
  nth(1) |> 
  arrange(x)

boot_straps |> 
  pull(strap_sample) |> 
  nth(2) |> 
  arrange(x)
```

Now we do the 'lm' fit

```{r}
boot_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(y ~ x, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  select(strap_number, results) |> 
  unnest(results)
```

Try to summarize these results = get a bootstrap

```{r}
#standard error of an estimate is the standard deviation of that estimate across repeated samples

#compute the standard deviation of the estimated coefficients
boot_results |> 
  group_by(term) |> 
  summarize(
    se = sd(estimate)
  )
#in a bootstrap, you should have less variability on the intercept and more variability in the slope comparied to the sim_df_nonconst
```


look at distribution 
```{r}
boot_results |> 
  filter(term == "x") |> 
  ggplot(aes(x=estimate)) +
  geom_density()
```

can I construct a CI

```{r}
#need more bootstrap samples to construct confidence intervals than to construct a se

boot_results |> 
  group_by(term) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.025)
  )
```





